{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will walk you through the basics of how to:\n",
    "- Create an \"Extractor\", a controller which handles the extraction.\n",
    "- Add required labelling functions and schemas to the extractor.\n",
    "- Run the extraction process.\n",
    "- Launch the user interface to begin annotating the extractions.\n",
    "\n",
    "We will be using the existing Keyword Extractor labelling function as an example.\n",
    "\n",
    "Begin by importing the requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/dev/Turing/elicit/examples\n",
      "Documents: ['doc_1.txt', 'doc_2.txt']\n"
     ]
    }
   ],
   "source": [
    "# Import Extractor class and launch UI function.\n",
    "from elicit import Extractor, launch_ui\n",
    "# Import the Keyword Match Labelling Function.\n",
    "from elicit.labelling_functions import KeywordMatchLF\n",
    "# Import Pathlib, for better path handling.\n",
    "from pathlib import Path\n",
    "import os\n",
    "current_path = os.path.abspath('')\n",
    "\n",
    "from elicit.main import _kill_ui\n",
    "\n",
    "# get current directory\n",
    "current_dir = Path(current_path)\n",
    "\n",
    "docs = list((current_dir / \"basic_example_docs\").glob(\"*.txt\"))\n",
    "\n",
    "print(\"Current directory:\", current_dir)\n",
    "print(\"Documents:\", [d.name for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first create an Extractor object, pointing at the DB file we want to create.\n",
    "Then, we will add the required schemas. These are the configuration files the labelling functions will use to extract the data.\n",
    "\n",
    "In this case, we require:\n",
    "- A categories schema\n",
    "- A keywords schema\n",
    "\n",
    "Categories is always required. It tells the system what categories each variable has, or whether it is numerical/raw. More details on this in the documentation.\n",
    "\n",
    "Keywords are a dictionary of variable category to keyword list. Each category of a variable will have some user-defined set of keywords.\n",
    "\n",
    "These schemas can either be a Path to a yaml file, or a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Extraction Database: /home/dev/Turing/elicit/examples/test_db.sqlite\n"
     ]
    }
   ],
   "source": [
    "# delete db if already exists (just for testing purposes)\n",
    "(current_dir / \"test_db.sqlite\").unlink(missing_ok=True)\n",
    "\n",
    "extractor = Extractor(db_path=current_dir / \"test_db.sqlite\")\n",
    "\n",
    "\n",
    "categories = {\"cat_or_dog\": [\"cat\", \"dog\"]}\n",
    "keywords = {\"cat_or_dog\": {\"cat\": [\"meow\", \"hiss\"], \"dog\": [\"woof\", \"bark\"]}}\n",
    "\n",
    "extractor.register_schema(schema=categories,\n",
    "                            schema_name=\"categories\")\n",
    "extractor.register_schema(schema=keywords,\n",
    "                            schema_name=\"keywords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we register the labelling function. In this case, we have just imported the pre-defined Keyword Extractor labelling function. In a future tutorial, we will create our own labelling functions.\n",
    "\n",
    "We can now run the extraction process, pointing to the directory containing the documents we want to extract from. Currently PDFs and TXTs are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LF: Keyword Match\n",
      "Loading models and stuff...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting variable: cat_or_dog: 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "extractor.register_labelling_function(KeywordMatchLF)\n",
    "extractor.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can launch the user interface to begin annotating the extractions, pointing at the database the extractions were just added to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Launched UI at: <a href=\"http://localhost:8080\">http://localhost:8080</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI Killed\n"
     ]
    }
   ],
   "source": [
    "launch_ui(db_path=current_dir / \"test_db.sqlite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('elicit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b172029f19524460cce6c4747fdf3f29a8b5708838bec7f17ceb06e5028447e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
