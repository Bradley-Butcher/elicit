{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elicit Basics\n",
    "\n",
    "This example will walk you through the basics of how to:\n",
    "- Create an \"Extractor\", a controller which handles the extraction.\n",
    "- Add required labelling functions and schemas to the extractor.\n",
    "- Run the extraction process.\n",
    "- Launch the user interface to begin annotating the extractions.\n",
    "\n",
    "We will be using the existing Keyword Extractor labelling function as an example.\n",
    "\n",
    "We begin by importing the requirements.\n",
    "\n",
    "## Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/miniconda3/envs/elicit/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dev/miniconda3/envs/elicit/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/dev/Turing/elicit/examples\n",
      "Documents: ['doc_1.txt', 'doc_2.txt']\n"
     ]
    }
   ],
   "source": [
    "# Import Extractor class and launch UI function.\n",
    "from elicit import Extractor, launch_ui\n",
    "# Import the Keyword Match Labelling Function.\n",
    "from elicit.generic_labelling_functions import KeywordMatchLF, NLILabellingFunction\n",
    "# Import Pathlib, for better path handling.\n",
    "from pathlib import Path\n",
    "# Import OS so we know where the notebook is!\n",
    "import os\n",
    "\n",
    "current_path = os.path.abspath('')\n",
    "\n",
    "# get current directory\n",
    "current_dir = Path(current_path)\n",
    "\n",
    "docs = list((current_dir / \"basic_example_docs\").glob(\"*.txt\"))\n",
    "\n",
    "print(\"Current directory:\", current_dir)\n",
    "print(\"Documents:\", [d.name for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Extractor\n",
    "\n",
    "Lets first create an Extractor object, pointing at the DB file we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Extraction Database: /home/dev/Turing/elicit/examples/test_db.sqlite\n"
     ]
    }
   ],
   "source": [
    "# delete db if already exists (just for testing purposes)\n",
    "#(current_dir / \"test_db.sqlite\").unlink(missing_ok=True)\n",
    "\n",
    "extractor = Extractor(db_path=current_dir / \"test_db.sqlite\", model_path=current_dir / \"models\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Schemas\n",
    "\n",
    "Next, we will add the required schemas. These are the configuration files the labelling functions will use to extract the data.\n",
    "\n",
    "In this case, we require:\n",
    "- A categories schema\n",
    "- A keywords schema\n",
    "\n",
    "Categories is always required. It tells the system what categories each variable has, or whether it is numerical/raw. More details on this in the documentation.\n",
    "\n",
    "Keywords are a dictionary of variable category to keyword list. Each category of a variable will have some user-defined set of keywords.\n",
    "\n",
    "These schemas can either be a Path to a yaml file, or a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered schema: categories\n",
      "Registered schema: keywords\n",
      "Registered schema: questions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = {\"cat_or_dog\": [\"cat\", \"dog\"]}\n",
    "keywords = {\"cat_or_dog\": {\"cat\": [\"meow\", \"hiss\"], \"dog\": [\"woof\", \"bark\"]}}\n",
    "questions = {\"cat_or_dog\": [\"Is this a cat?\", \"Is this a dog?\"]}\n",
    "\n",
    "extractor.register_schema(schema=categories,\n",
    "                            schema_name=\"categories\")\n",
    "extractor.register_schema(schema=keywords,\n",
    "                            schema_name=\"keywords\")#\n",
    "extractor.register_schema(schema=questions,\n",
    "                            schema_name=\"questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Labelling Functions\n",
    "\n",
    "Next, we register the labelling function. In this case, we have just imported the pre-defined Keyword Extractor labelling function. In a future tutorial, we will create our own labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered labelling function: Keyword Match\n",
      "Registered labelling function: Q&A → NLI Transformer\n"
     ]
    }
   ],
   "source": [
    "extractor.register_labelling_function(KeywordMatchLF)\n",
    "extractor.register_labelling_function(NLILabellingFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the extractor\n",
    "\n",
    "We can now run the extraction process, we pass a list of Paths pointing to each document. Currently PDFs and TXTs are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the user interface\n",
    "\n",
    "Finally, we can launch the user interface to begin annotating the extractions, pointing either to a database path, or simply passing in the extractor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Launched UI at: <a href=\"http://localhost:8080\">http://localhost:8080</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI Killed\n"
     ]
    }
   ],
   "source": [
    "launch_ui(extractor=extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once some extractions have been validated - labelling functions can be fine-tuned with `extractor.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Resources for LF: Keyword Match\n",
      "Training LF: Keyword Match on variable: cat_or_dog\n",
      "2 documents with validations for variable: cat_or_dog\n",
      "Loading Resources for LF: Q&A → NLI Transformer\n",
      "Fine tuned Sequence Classifier model found, loading...\n",
      "Fine tuned Q&A model found, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RobertaForQuestionAnsweringWithNegatives' is not supported for question-answering. Supported models are ['QDQBertForQuestionAnswering', 'FNetForQuestionAnswering', 'GPTJForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'RemBertForQuestionAnswering', 'CanineForQuestionAnswering', 'RoFormerForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BigBirdForQuestionAnswering', 'ConvBertForQuestionAnswering', 'LEDForQuestionAnswering', 'DistilBertForQuestionAnswering', 'AlbertForQuestionAnswering', 'CamembertForQuestionAnswering', 'BartForQuestionAnswering', 'MBartForQuestionAnswering', 'LongformerForQuestionAnswering', 'XLMRobertaForQuestionAnswering', 'RobertaForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'BertForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'FlaubertForQuestionAnsweringSimple', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'ElectraForQuestionAnswering', 'ReformerForQuestionAnswering', 'FunnelForQuestionAnswering', 'LxmertForQuestionAnswering', 'MPNetForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'IBertForQuestionAnswering', 'SplinterForQuestionAnswering'].\n",
      "***** Running training *****\n",
      "  Num examples = 2\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LF: Q&A → NLI Transformer on variable: cat_or_dog\n",
      "2 documents with validations for variable: cat_or_dog\n",
      "Training Seq. Classification Model\n",
      "Fine-tuning zero-shot sequence classifier for variable cat_or_dog on 2 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in /home/dev/Turing/elicit/examples/models/seq_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained Model to /home/dev/Turing/elicit/examples/models/seq_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/dev/Turing/elicit/examples/models/seq_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "extractor.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LF: Keyword Match\n",
      "Loading Resources.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting variable: cat_or_dog: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LF: Q&A → NLI Transformer\n",
      "Loading Resources.\n",
      "Fine tuned Sequence Classifier model found, loading...\n",
      "Fine tuned Q&A model found, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RobertaForQuestionAnsweringWithNegatives' is not supported for question-answering. Supported models are ['QDQBertForQuestionAnswering', 'FNetForQuestionAnswering', 'GPTJForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'RemBertForQuestionAnswering', 'CanineForQuestionAnswering', 'RoFormerForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BigBirdForQuestionAnswering', 'ConvBertForQuestionAnswering', 'LEDForQuestionAnswering', 'DistilBertForQuestionAnswering', 'AlbertForQuestionAnswering', 'CamembertForQuestionAnswering', 'BartForQuestionAnswering', 'MBartForQuestionAnswering', 'LongformerForQuestionAnswering', 'XLMRobertaForQuestionAnswering', 'RobertaForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'BertForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'FlaubertForQuestionAnsweringSimple', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'ElectraForQuestionAnswering', 'ReformerForQuestionAnswering', 'FunnelForQuestionAnswering', 'LxmertForQuestionAnswering', 'MPNetForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'IBertForQuestionAnswering', 'SplinterForQuestionAnswering'].\n",
      "Extracting variable: cat_or_dog: 100%|██████████| 2/2 [00:00<00:00,  7.39it/s]\n"
     ]
    }
   ],
   "source": [
    "extractor.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('elicit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b172029f19524460cce6c4747fdf3f29a8b5708838bec7f17ceb06e5028447e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
