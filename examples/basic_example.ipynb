{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elicit Basics\n",
    "\n",
    "This example will walk you through the basics of how to:\n",
    "- Create an \"Extractor\", a controller which handles the extraction.\n",
    "- Add required labelling functions and schemas to the extractor.\n",
    "- Run the extraction process.\n",
    "- Launch the user interface to begin annotating the extractions.\n",
    "\n",
    "We will be using the existing Keyword Extractor labelling function as an example.\n",
    "\n",
    "We begin by importing the requirements.\n",
    "\n",
    "## Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/miniconda3/envs/elicit/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dev/miniconda3/envs/elicit/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/dev/Turing/elicit/examples\n",
      "Documents: ['doc_1.txt', 'doc_2.txt']\n"
     ]
    }
   ],
   "source": [
    "# Import Extractor class and launch UI function.\n",
    "from elicit import Extractor, launch_ui\n",
    "# Import the Keyword Match Labelling Function.\n",
    "from elicit.generic_labelling_functions import KeywordMatchLF, NLILabellingFunction\n",
    "# Import Pathlib, for better path handling.\n",
    "from pathlib import Path\n",
    "# Import OS so we know where the notebook is!\n",
    "import os\n",
    "\n",
    "current_path = os.path.abspath('')\n",
    "\n",
    "# get current directory\n",
    "current_dir = Path(current_path)\n",
    "\n",
    "docs = list((current_dir / \"basic_example_docs\").glob(\"*.txt\"))\n",
    "\n",
    "print(\"Current directory:\", current_dir)\n",
    "print(\"Documents:\", [d.name for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Extractor\n",
    "\n",
    "Lets first create an Extractor object, pointing at the DB file we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Extraction Database: /home/dev/Turing/elicit/examples/test_db.sqlite\n"
     ]
    }
   ],
   "source": [
    "# delete db if already exists (just for testing purposes)\n",
    "#(current_dir / \"test_db.sqlite\").unlink(missing_ok=True)\n",
    "\n",
    "extractor = Extractor(db_path=current_dir / \"test_db.sqlite\", model_path=current_dir / \"models\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Schemas\n",
    "\n",
    "Next, we will add the required schemas. These are the configuration files the labelling functions will use to extract the data.\n",
    "\n",
    "In this case, we require:\n",
    "- A categories schema\n",
    "- A keywords schema\n",
    "\n",
    "Categories is always required. It tells the system what categories each variable has, or whether it is numerical/raw. More details on this in the documentation.\n",
    "\n",
    "Keywords are a dictionary of variable category to keyword list. Each category of a variable will have some user-defined set of keywords.\n",
    "\n",
    "These schemas can either be a Path to a yaml file, or a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered schema: categories\n",
      "Registered schema: keywords\n",
      "Registered schema: questions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = {\"cat_or_dog\": [\"cat\", \"dog\"]}\n",
    "keywords = {\"cat_or_dog\": {\"cat\": [\"meow\", \"hiss\"], \"dog\": [\"woof\", \"bark\"]}}\n",
    "questions = {\"cat_or_dog\": [\"Is this a cat?\", \"Is this a dog?\"]}\n",
    "\n",
    "extractor.register_schema(schema=categories,\n",
    "                            schema_name=\"categories\")\n",
    "extractor.register_schema(schema=keywords,\n",
    "                            schema_name=\"keywords\")#\n",
    "extractor.register_schema(schema=questions,\n",
    "                            schema_name=\"questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Labelling Functions\n",
    "\n",
    "Next, we register the labelling function. In this case, we have just imported the pre-defined Keyword Extractor labelling function. In a future tutorial, we will create our own labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered labelling function: Keyword Match\n",
      "Registered labelling function: Q&A → NLI Transformer\n"
     ]
    }
   ],
   "source": [
    "extractor.register_labelling_function(KeywordMatchLF)\n",
    "extractor.register_labelling_function(NLILabellingFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the extractor\n",
    "\n",
    "We can now run the extraction process, we pass a list of Paths pointing to each document. Currently PDFs and TXTs are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the user interface\n",
    "\n",
    "Finally, we can launch the user interface to begin annotating the extractions, pointing either to a database path, or simply passing in the extractor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch_ui(extractor=extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Resources for LF: Keyword Match\n",
      "Training LF: Keyword Match on variable: cat_or_dog\n",
      "3 documents with validations for variable: cat_or_dog\n",
      "Loading Resources for LF: Q&A → NLI Transformer\n",
      "No fine tuned Q&A model found, loading generic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RobertaForQuestionAnsweringWithNegatives' is not supported for question-answering. Supported models are ['QDQBertForQuestionAnswering', 'FNetForQuestionAnswering', 'GPTJForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'RemBertForQuestionAnswering', 'CanineForQuestionAnswering', 'RoFormerForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BigBirdForQuestionAnswering', 'ConvBertForQuestionAnswering', 'LEDForQuestionAnswering', 'DistilBertForQuestionAnswering', 'AlbertForQuestionAnswering', 'CamembertForQuestionAnswering', 'BartForQuestionAnswering', 'MBartForQuestionAnswering', 'LongformerForQuestionAnswering', 'XLMRobertaForQuestionAnswering', 'RobertaForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'BertForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'FlaubertForQuestionAnsweringSimple', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'ElectraForQuestionAnswering', 'ReformerForQuestionAnswering', 'FunnelForQuestionAnswering', 'LxmertForQuestionAnswering', 'MPNetForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'IBertForQuestionAnswering', 'SplinterForQuestionAnswering'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LF: Q&A → NLI Transformer on variable: cat_or_dog\n",
      "3 documents with validations for variable: cat_or_dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.63it/s, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained Model to /home/dev/Turing/elicit/examples/models/qna_model\n"
     ]
    }
   ],
   "source": [
    "extractor.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LF: Keyword Match\n",
      "Loading Resources.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting variable: cat_or_dog:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such column: alert",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Turing/elicit/elicit/interface.py\u001b[0m in \u001b[0;36m_push_evidence\u001b[0;34m(self, document_id, variable_id, extraction, method)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             self.db.execute(\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \"INSERT INTO extraction (extraction_id, method, exact_context, local_context, wider_context, confidence, variable_id, document_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\", (next_extraction_id, method, extraction.exact_context, extraction.local_context, extraction.wider_context, extraction.confidence, variable_id, document_id))\n",
      "\u001b[0;31mIntegrityError\u001b[0m: UNIQUE constraint failed: extraction.extraction_id",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10911/214375366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Turing/elicit/elicit/extractor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     pbar.set_description(\n\u001b[1;32m     95\u001b[0m                         f\"Extracting variable: {variable}\")\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mlf_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;31m# free up memory from models and stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mlf_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Turing/elicit/elicit/generic_labelling_functions/keyword_search.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, document_name, variable_name, document_text)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keywords\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         )\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Turing/elicit/elicit/interface.py\u001b[0m in \u001b[0;36mpush_many\u001b[0;34m(self, document_name, variable_name, extraction_list)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mextraction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextraction_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Turing/elicit/elicit/interface.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, document_name, variable_name, extraction)\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise ValueError(\"Value {} is not a valid category for variable {}.\".format(\n\u001b[1;32m    206\u001b[0m                 extraction.value, variable_name))\n\u001b[0;32m--> 207\u001b[0;31m         self.logger.push(document_name, variable_name,\n\u001b[0m\u001b[1;32m    208\u001b[0m                          extraction, self.labelling_method)\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Turing/elicit/elicit/interface.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, document_name, variable_name, extraction, method)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mvar_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_push_evidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Turing/elicit/elicit/interface.py\u001b[0m in \u001b[0;36m_push_evidence\u001b[0;34m(self, document_id, variable_id, extraction, method)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT valid from extraction where extraction_id = ?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_extraction_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'TRUE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             self.db.execute(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \"UPDATE extraction SET method = ?, exact_context = ?, local_context = ?, wider_context = ?, confidence = ?, alert = ? WHERE extraction_id = ?\", (method, extraction.exact_context, extraction.local_context, extraction.wider_context, extraction.confidence, 'TRUE', next_extraction_id))\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such column: alert"
     ]
    }
   ],
   "source": [
    "extractor.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('elicit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b172029f19524460cce6c4747fdf3f29a8b5708838bec7f17ceb06e5028447e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
